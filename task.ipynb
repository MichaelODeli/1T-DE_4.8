{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "import datetime\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"jupyter\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = \"notebook\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа со Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем сессию   \n",
    "Будет доступна админка по ссылке [http://localhost:4040/](http://localhost:4040/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[6]\") \\\n",
    "      .appName(\"SparkSecond\") \\\n",
    "      .config(\"spark.driver.memory\", \"8g\")\\\n",
    "      .getOrCreate() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заполняем фрейм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+\n",
      "| id| timestamp|          date_time|\n",
      "+---+----------+-------------------+\n",
      "|  1|1562007679|2019-07-02 00:01:19|\n",
      "|  1|1562007710|2019-07-02 00:01:50|\n",
      "|  1|1562007720|2019-07-02 00:02:00|\n",
      "|  1|1562007750|2019-07-02 00:02:30|\n",
      "|  2|1564682430|2019-08-01 23:00:30|\n",
      "|  2|1564682450|2019-08-01 23:00:50|\n",
      "|  2|1564682480|2019-08-01 23:01:20|\n",
      "+---+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['id', 'timestamp']\n",
    "data = [[1, 1562007679], [1, 1562007710], [1, 1562007720], [1, 1562007750], [2, 1564682430], [2, 1564682450], [2, 1564682480]]\n",
    "first_df = spark.createDataFrame(data).toDF(*columns)\n",
    "first_df = first_df.withColumn('date_time', func.from_unixtime('timestamp').cast(typ.TimestampType()))\n",
    "first_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Получаем длительность сессии в секундах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| id|session_in_secs|\n",
      "+---+---------------+\n",
      "|  1|             71|\n",
      "|  2|             50|\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diff_df = first_df.groupBy('id').\\\n",
    "    agg(func.max('date_time').alias('max_date'), func.min('date_time').alias('min_date'))\\\n",
    "    .withColumn('session_in_secs', func.col('max_date').cast(\"long\") - func.col('min_date').cast(\"long\")).select('id', 'session_in_secs')\n",
    "diff_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заполняем фреймы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+\n",
      "|product|location|demand|\n",
      "+-------+--------+------+\n",
      "|      1|       1|   100|\n",
      "|      1|       2|   110|\n",
      "|      2|       1|   120|\n",
      "|      2|       2|    90|\n",
      "|      3|       1|    70|\n",
      "|      3|       2|    80|\n",
      "+-------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['product', 'location', 'demand']\n",
    "data = [[1, 1, 100], [1, 2, 110], [2, 1, 120], [2, 2, 90], [3, 1, 70], [3, 2, 80]]\n",
    "first_df = spark.createDataFrame(data).toDF(*columns)\n",
    "first_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----+\n",
      "|product|location|stock|\n",
      "+-------+--------+-----+\n",
      "|      1|       1| 1000|\n",
      "|      1|       2|  400|\n",
      "|      2|       1|  300|\n",
      "|      2|       2|  250|\n",
      "+-------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['product', 'location', 'stock']\n",
    "data = [[1, 1, 1000], [1, 2, 400], [2, 1, 300], [2, 2, 250]]\n",
    "second_df = spark.createDataFrame(data).toDF(*columns)\n",
    "second_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Объединим датафреймы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+-----+\n",
      "|product|location|demand|stock|\n",
      "+-------+--------+------+-----+\n",
      "|      1|       1|   100| 1000|\n",
      "|      1|       2|   110|  400|\n",
      "|      2|       1|   120|  300|\n",
      "|      2|       2|    90|  250|\n",
      "+-------+--------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df = first_df.join(second_df, ['product', 'location'])\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создадим еще фрейм с номерами недель и кол-во дней в них"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|week|days|\n",
      "+----+----+\n",
      "|   0|   4|\n",
      "|   1|   7|\n",
      "|   2|   7|\n",
      "|   3|   7|\n",
      "|   4|   5|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['week', 'days']\n",
    "data = [[0, 4], [1, 7], [2, 7], [3, 7], [4, 5]]\n",
    "third_df = spark.createDataFrame(data).toDF(*columns)\n",
    "third_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Готовим витрину"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----+----------+---------------+\n",
      "|product|location|week|sales_calc|remaining_stock|\n",
      "+-------+--------+----+----------+---------------+\n",
      "|      1|       1|   0|       400|            600|\n",
      "|      1|       2|   0|       440|            -40|\n",
      "|      2|       1|   0|       480|           -180|\n",
      "|      2|       2|   0|       360|           -110|\n",
      "|      1|       1|   1|       700|            300|\n",
      "|      1|       2|   1|       770|           -370|\n",
      "|      2|       1|   1|       840|           -540|\n",
      "|      2|       2|   1|       630|           -380|\n",
      "|      1|       1|   2|       700|            300|\n",
      "|      1|       2|   2|       770|           -370|\n",
      "|      2|       1|   2|       840|           -540|\n",
      "|      2|       2|   2|       630|           -380|\n",
      "|      1|       1|   3|       700|            300|\n",
      "|      1|       2|   3|       770|           -370|\n",
      "|      2|       1|   3|       840|           -540|\n",
      "|      2|       2|   3|       630|           -380|\n",
      "|      1|       1|   4|       500|            500|\n",
      "|      1|       2|   4|       550|           -150|\n",
      "|      2|       1|   4|       600|           -300|\n",
      "|      2|       2|   4|       450|           -200|\n",
      "+-------+--------+----+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.createOrReplaceTempView(\"sales_data\")\n",
    "third_df.createOrReplaceTempView(\"weeks\")\n",
    "vitrina = spark.sql(\"select * from sales_data, weeks\").withColumns({\n",
    "    'sales_calc': func.col('demand') * func.col('days'),\n",
    "    'remaining_stock': func.col('stock') - func.col('sales_calc')\n",
    "}).select('product', 'location', 'week', 'sales_calc', 'remaining_stock')\n",
    "vitrina.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a4aca0c14410736538bc22f4c63fcff0fb2119c2ddd24e186890533091678b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
